{% docs __snowplow_web__ %}

{% raw %}

# Snowplow Web Package

Welcome to the documentation site for the Snowplow web dbt package. The package contains is a fully incremental model, that transforms raw web event data generated by the [Snowplow JavaScript tracker](https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/javascript-trackers/) into a series of derived tables of varying levels of aggregation.

## Overview

This model consists of a series of modules, each producing a table which serves as the input to the next module. The 'standard' modules are:

- Base: Performs the incremental logic, outputting the table `snowplow_web_base_events_this_run` which contains a de-duped data set of all events required for the current run of the model.
- Page Views: Aggregates event level data to a page view level, `page_view_id`.
- Sessions: Aggregates page view level data to a session level, `domain_sessionid`.
- Users: Aggregates session level data to a users level, `domain_userid`.

The 'standard' modules can be thought of as source code for the core logic of the model, which Snowplow maintains. These modules carry out the incremental logic in such a way as custom modules can be written to plug into the model's structure, without needing to write a parallel incremental logic. We recommend that all customisations are written in this way, which allows us to safely maintain and roll out updates to the model, without impact on dependent custom sql.

Each module produces a table which acts as the input to the subsequent module (the `_this_run` tables), and updates a derived table - with the exception of the Base module, which takes atomic data as its input, and does not update a derived table.

## Adapter Support

The Snowplow Web v.0.3.0 package currently supports BigQuery, Redshift, Snowflake & Postgres.

## Installation

Check [dbt Hub](https://hub.getdbt.com/snowplow/snowplow_web/latest/) for the latest installation instructions, or read the [dbt docs][dbt-package-docs] for more information on installing packages.

## Quick Start

### 1 - Check source data

This package will by default will assume your Snowplow events data is contained in the `atomic` schema of your [target.database](https://docs.getdbt.com/docs/running-a-dbt-project/using-the-command-line-interface/configure-your-profile). In order to change this, please add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__atomic_schema: schema_with_snowplow_events
    snowplow__database: database_with_snowplow_events
```

### 2 - Enabled desired contexts

The web package has the option to join in data from the following 3 Snowplow enrichments:

- [IAB enrichment](https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/iab-enrichment/)
- [UA Parser enrichment](https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/ua-parser-enrichment/)
- [YAUAA enrichment](https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/yauaa-enrichment/)

By default these are **all disabled** in the web package. Assuming you have the enrichments turned on in your Snowplow pipeline, to enable the contexts within the package please add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__enable_iab: true
    snowplow__enable_ua: true
    snowplow__enable_yauaa: true
```

### 3 - Filter your data set

You can specify both `start_date` at which to start processing events and the `app_id`'s to filter for. By default the `start_date` is set to `2020-01-01` and all `app_id`'s are selected. To change this please add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__start_date: 'yyyy-mm-dd'
    snowplow__app_id: ['my_app_1','my_app_2']
```

#### BigQuery Only

Verify which column your events table is partitioned on. It will likely be partitioned on `collector_tstamp` or `derived_tstamp`. If it is partitioned on `collector_tstamp` you should set `snowplow__derived_tstamp_partitioned` to `false`. This will ensure only the `collector_tstamp` column is used for partition pruning when querying the events table:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__derived_tstamp_partitioned: false
```

### 4 - Verify page ping variables

The web package processes page ping events to calculate web page engagement times. If your [tracker configuration](https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/javascript-trackers/javascript-tracker/javascript-tracker-v3/tracking-events/#activity-tracking-page-pings) for `min_visit_length` (default 5) and `heartbeat` (default 10) differs from the defaults provided in this package, you can override by adding to your `dbt_project.yml`:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__min_visit_length: 5 # Default value
    snowplow__heartbeat: 10 # Default value
```

## Configuration

### Output Schemas

#### Scratch & Derived Tables

By default all scratch/staging tables will be created in the `<target.schema>_scratch` schema while the derived tables (`snowplow_web_page_views`, `snowplow_web_sessions`, `snowplow_web_users`) will be created in `<target.schema>_derived`. To change either, please add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
models:
  snowplow_web:
    base:
      scratch:
        +schema: my_scratch_schema
    page_views:
      +schema: my_derived_schema
      scratch:
        +schema: my_scratch_schema
    sessions:
      +schema: my_derived_schema
      scratch:
        +schema: my_scratch_schema
    users:
      +schema: my_derived_schema
      scratch:
        +schema: my_scratch_schema
```

#### Manifest Tables

By default the manifest tables generated by this package will be created in the schema `<target.schema>._snowplow_manifest`. To override the `snowplow_manifest` suffix, add the following to your `dbt_project.yml` file:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__manifest_custom_schema: "your_snowplow_manifest_schema"
```

**Note: Issues can occur by changing the manifest schema by any other means aside from setting the `snowplow__manifest_custom_schema` variable.**

### Disabling a standard module

If you do not require certain modules provided by the package you have the option to disable them. For instance to disable the users module:

```yml
# dbt_project.yml
...
models:
  snowplow_web:
    users:
      enabled: false
```

Note that any dependent modules will also need to be disabled - for instance if you disabled the sessions module, you will also have to disable the users module.

### Further Configuration

This package makes use of a series of other variables, which are all set to the recommend values for the operation of the web model. Depending on your use case, you might want to override these values by adding to your `dbt_project.yml` file.

`snowplow__lookback_window_hours`:  Default 6. The number of hours to look before the latest event processed - to account for late arriving data, which comes out of order.

`snowplow__backfill_limit_days`:    Default 30. The maximum numbers of days of new data to be processed since the latest event processed. Please refer to the back-filling section for more details.

`snowplow__days_late_allowed`:      Default 3. The maximum allowed number of days between the event creation and it being sent to the collector. Exists to reduce lengthy table scans that can occur as a result of late arriving data.

`snowplow__max_session_days`:       Default 3. The maximum allowed session length in days. For a session exceeding this length, all events after this limit will stop being processed. Exists to reduce lengthy table scans that can occur due to long sessions which are usually a result of bots.

`snowplow__upsert_lookback_days`:   Default 30. Number of day to look back over the incremental derived tables during the upsert. Where performance is not a concern, should be set to as long a value as possible. Having too short a period can result in duplicates. Please see the incremental materialization section for more details.

`snowplow__ua_bot_filter`:          Default `True`. Configuration to filter out bots via the useragent string pattern match.

`snowplow__sessions_table`:         Default `{{ ref('snowplow_web_sessions') }}`. The users module requires data from the derived sessions table. If you choose to disable the standard sessions table in favor of your own custom table, set this to reference your new table e.g. `{{ ref('snowplow_web_sessions_custom') }}`. Please see the [README](https://github.com/snowplow/dbt-snowplow-web/tree/main/custom_example) in the `custom_example` directory for more information on this sort of implementation.

`snowplow__has_log_enabled`:        Default `true`. When executed, the package logs information about the current run to the CLI. This can be disabled by setting to `false`.

`snowplow__incremental_materialization`: Default `snowplow_incremental`. The materialization used for all incremental models within the package. `snowplow_incremental` builds upon the default incremental materialization provided by dbt, improving performance when modeling event data. If however you prefer to use the native dbt incremental materialization, or any other, then adjust accordingly.

## YAML Selectors

Within this package we have provided a suite of suggested selectors to run and test the models within the package. This leverages dbt's [selector flag][dbt-selectors].

The selectors include:

- `snowplow_web`: Recommended way to run the package. This selection includes all models within the Snowplow Web as well as any custom models you have created.
- `snowplow_web_lean_tests`: Recommended way to test the models within the package. See the testing section for more details.

These are defined in the [`selectors.yml` file][selectors-yml-file] within the package, however in order to use these selections you will need to copy this file into your own dbt project directory. This is a top-level file and therefore should sit alongside your `dbt_project.yml` file.

## Operation

The Snowplow web model is designed to be run as a whole, which ensures all incremental tables are kept in sync. As such, we suggest running the model using:

```bash
dbt run --models snowplow_web tag:snowplow_web_incremental
```

The `snowplow_web` selection will execute all nodes within the Snowplow web package, while the `tag:snowplow_web_incremental` will execute all custom modules that you may have created.

Given the verbose nature of this command we suggest using the YAML selectors we have provided (see section above). The equivalent command using the selector flag would be:

```bash
dbt run --selector snowplow_web
```

### Complete refresh of Snowplow web package

While you can drop and recompute the incremental tables within this package using the standard `--full-refresh` flag, the history of events consumed by each model will be retained in the `snowplow_web_incremental_manifest` table. Due to the critical nature of this table, it is protected from being dropped accidentally during a full refresh. Without dropping the manifest during a full refresh, the selected incremental tables would be dropped but the processing of events would resume from where the model left off rather than your `snowplow__start_date`.

In order to tear down all the manifest tables and start again set the `teardown_all` var to `true` at run time:

```bash
dbt run --models snowplow_web tag:snowplow_web_incremental --full-refresh --vars 'teardown_all: true'
# or using selector flag
dbt run --selector snowplow_web --full-refresh --vars 'teardown_all: true'
```

### Back-filling custom modules

Overtime you may wish to add custom modules to extend the functionality of this package. As you introduce new custom modules into your project, assuming they are tagged correctly (see section on custom modules), the web model will automatically replay all events up until the latest event to have been processed by the other modules.

Note that the batch size of this back-fill is limited as outlined in the 'identification of events to process' section. This means it might take several runs to complete the back-fill, **during which time no new events will be processed by the web model**.

During back-filling, the derived page views, sessions and users tables are blocked from updating. This is to protect against a batched back-fill temporarily introducing incomplete data into these derived tables.

Back-filling a module can be performed either as part of the entire run of the Snowplow web package, or in isolation to reduce cost (recommended):

```bash
dbt run --models snowplow_web tag:snowplow_web_incremental # Will execute all Snowplow web modules, as well as custom.
dbt run --models +my_custom_module # Will execute only your custom module + any upstream nodes.
```

### Tearing down a subset of models

As the code base for your custom modules evolves, you will likely need to replay events through a given module. In order to do so, the models within your custom module need to be removed from the `snowplow_web_incremental_manifest` table. See the 'Complete refresh' section for an explanation as to why. This removal can be achieved by passing the model's name to the `models_to_remove'` var at run time. If you want to replay events through a series of dependent models, you only need to pass the name of the endmost model within the run:

```bash
dbt run --models +snowplow_web_custom_incremental_model --full-refresh --vars 'models_to_remove: snowplow_web_custom_incremental_model'
```

By removing the `snowplow_web_custom_incremental_model` model from the manifest the web packages will be in state 2 and will replay all events.

## Tests

This package contains tests for both the scratch and derived models. Depending on your use case you might not want to run all tests in production, for example to save costs. There are several tags included in the package to help select subsets of tests. Tags:

- `this_run`: Any model with the `_this_run` suffix
- `scratch`: Any model in the scratch sub directories.
- `derived`: Any of the derived models i.e. page views, sessions and users.
- `primary-key`: Any test on the primary keys of all models in this package.

For example if your derived tables are very large you may want to run the full test suite on the `this_run` tables, which act as the input for the derived tables, but only primary key schema tests on the derived tables to ensure no duplicates. If using such a set up, we would also recommend including the `page_view_in_session_value` data test for the page views derived tables.

This is our recommended approach to testing and can be implemented using the selector flag (see YAML selectors section for more details) as follows:

```bash
dbt test --selector snowplow_web_lean_tests
```

This is equivalent to:

```bash
dbt test --models snowplow_web,tag:this_run # Full tests on _this_run models
dbt test --models snowplow_web,tag:manifest # Full tests on manifest models
dbt test --models snowplow_web,tag:primary-key,tag:derived # Primary key tests only on derived tables.
dbt test --models snowplow_web,tag:derived,test_type:data  # Include the page_view_in_session_value data test
```

Alternatively, if you wanted to run all available tests in both the Snowplow web package and your custom modules:

```bash
dbt test --selector snowplow_web
```

## Incremental Logic

This package uses a centralized manifest table, `snowplow_web_incremental_manifest`, to record what events have already been processed and by which model/node. This allows for easy identification of what events to process in subsequent runs of the package. The manifest table is updated as part of an `on-run-end` hook, which calls the `snowplow_incremental_post_hook()` macro.

Example `snowplow_web_incremental_manifest`:

| model                            | last_success |
|----------------------------------|--------------|
| snowplow_web_page_views_this_run | '2021-06-03' |
| snowplow_web_page_views          | '2021-06-03' |
| snowplow_web_sessions            | '2021-06-02' |

In addition to the `snowplow_web_incremental_manifest` manifest table we also have the `snowplow_web_base_sessions_lifecycle_manifest` manifest, which keep a record of the lifecycle of all sessions. Both tables sit within `snowplow_manifest` schema and are protected from full refreshes.

### Identification of events to process

The identification of which events to process is performed by a pre-hook on the `snowplow_web_base_new_event_limits` model, which calls the `snowplow_incremental_pre_hook()` macro. This macro uses the metadata recorded in `snowplow_web_incremental_manifest` to determine the correct events to process next based on the current state of the Snowplow dbt Web model. The selection of these events is done by specifying a range of `collector_tstamp`'s to process, between `lower_limit` and `upper_limit`. The calculation of these limits is as follows.

First we query `snowplow_web_incremental_manifest`, filtering for all enabled models tagged with `snowplow_web_incremental` within your dbt project:

```sql
select min(last_success) as min_last_success,
       max(last_success) as max_last_success,
       coalesce(count(*), 0) as models
from snowplow_web_incremental_manifest
where model in (array_of_snowplow_tagged_enabled_models)
```

Based on the results the web model enters 1 of 4 states.

#### State 1: First run of the package

The query returns `models = 0` indicating that no models exist in the manifest.

`lower_limit: snowplow__start_date`  
`upper_limit: least(current_tstamp, snowplow__start_date + snowplow__backfill_limit_days)`  

#### State 2: New model introduced

`models < size(array_of_snowplow_tagged_enabled_models)` and therefore a new model, tagged with `snowplow_web_incremental`, has been added since the last run. The package will replay all previously processed events in order to back-fill the new model.

`lower_limit: snowplow__start_date`  
`upper_limit: least(max_last_success, snowplow__start_date + snowplow__backfill_limit_days)`  

#### State 3: Models out of sync

`min_last_success < max_last_success` and therefore the tagged models are out of sync, for example due to a particular model failing to execute successfully during the previous run. The package will attempt to sync all models.

`lower_limit: min_last_success - snowplow__lookback_window_hours`  
`upper_limit: least(max_last_success, min_last_success + snowplow__backfill_limit_days)`  

#### State 4: Standard run

If none of the above criteria are met, then we consider it a 'standard run' and we carry on from the last processed event.

`lower_limit: max_last_success - snowplow__lookback_window_hours`  
`upper_limit: least(current_tstamp, max_last_success + snowplow__backfill_limit_days)`  

**Note in all cases the `upper_limit` is limited by the `snowplow__backfill_limit_days` variable. This protects against back-fills with many rows causing very long run times.**

If you want to check the current state of the web model, run the `snowplow_web_base_new_event_limits` model. This will log the current state to the CLI while causing no disruption to the incremental processing of events.

```bash
dbt run --models snowplow_web_base_new_event_limits
...
00:26:28 | 1 of 1 START table model scratch.snowplow_web_base_new_event_limits.. [RUN]
00:26:29 + Snowplow: Standard incremental run
00:26:29 + Snowplow: Processing data between 2021-01-05 17:59:32 and 2021-01-07 23:59:32
```

## Custom Modules

This package is designed to be easily customised or extended within your own dbt project, by building your own 'custom modules'. The 'standard modules' we provide (base, page views, sessions and users) are not designed to be modified by you. An example dbt project with custom modules can be seen in the [custom example directory](https://github.com/snowplow/dbt-snowplow-web/tree/main/custom_example) of the snowplow-web repo.

### Guidelines & Best Practice

The Snowplow web package's modular structure allows for custom SQL modules to leverage the model's incrementalisation logic, and operate as 'plugins' to compliment the standard model. This can be achieved by using the `_this_run` tables as an input, and producing custom tables which may join to the standard model's main derived tables (for example, to aggregate custom contexts to a page_view level), or provide a separate level of aggregation (for example a custom user interaction).

The standard modules carry out the heavy lifting in establishing an incremental structure and providing the core logic for the most common web aggregation use cases. It also allows custom modules to be plugged in without impeding the maintenance of standard modules.

The following best practices should be followed to ensure that updates and bug fixes to the model can be rolled out with minimal complication:

- Custom modules should not modify any of the tables generated by the Snowplow web package e.g. the scratch, derived or manifest tables.
- Customisations should not modify the SQL provided by the package - they should only comprise of a new set of SQL statements, which produce a separate table.
- The logic for custom SQL should be idempotent, and restart-safe - in other words, it should be written in such a way that a failure mid-way, or a re-run of the model will not change the deterministic output.

In short, the standard modules can be treated as the source code for a distinct piece of software, and custom modules can be treated as self-maintained, additive plugins - in much the same way as a Java package may permit one to leverage public classes in their own API, and provide an entry point for custom programs to run, but will not permit one to modify the original API.

The `_this_run` and derived (e.g. `snowplow_web_page_views`, `snowplow_web_sessions`, `snowplow_web_users`) tables are considered part of the 'public' class of tables in this model structure, and so we can give assurances that non-breaking releases won't alter them. The other tables may be used in custom SQL, but their logic and structure may change from release to release, or they may be removed. If one does use a scratch table in custom logic, any breaking changes can be mitigated by either amending the custom logic to suit, or copying the relevant steps from an old version of the model into the custom module. (However this will rarely be necessary).

### What denotes a custom module?

**Does:**  
In short, anything that plugs into the incremental framework provided by this package. Generally speaking any models you create that reference any of the `_this_run` tables from the standard modules are leveraging this framework and therefore need to be tagged with `snowplow_web_incremental` (see the tagging section). Such models will typically be materialized as incremental, although for more complex custom modules there may be a series of staging models that ultimately produce a derived incremental model. In this case, all staging models also need to be tagged correctly.

**Doesn't:**  
Models that only reference a Snowplow web derived table as their input, rather than a `_this_run` table. Since these derived tables are materialized as incremental they contain all historic events. Any models you build that reference these tables can therefore by written in a drop and recompute manner i.e. materialized as a table. This means they do not leverage the incremental framework of this package and therefore **should not be tagged.**

### Inputs for custom modules

Listed below are the recommended tables to reference as your input for a custom module, depending on the level of aggregation required:

- Event level: `snowplow_web_base_events_this_run`
- Page view level: `snowplow_web_page_views_this_run`
- Session level: `snowplow_web_sessions_this_run`
- User level: `snowplow_web_users_this_run`

### Tagging models

All models within custom modules need to be tagged with `snowplow_web_incremental` in order to leverage the incremental logic of this package. We recommend creating a sub directory of your `/models` directory to contain all your custom modules. In this example we created the sub directory `snowplow_web_custom_modules`. We can then apply the tag to all models in this directory:

```yml
# dbt_project.yml
models:
  your_dbt_project:
    snowplow_web_custom_modules:
      +tags: snowplow_web_incremental #Adds tag to all models in the 'snowplow_web_custom_modules' directory
```

### Retiring Custom Modules

If you want to retire a custom module, you should:

- Delete the models from your project or [disable][dbt-disable-model] the models.
- Not worry about removing the models from the `snowplow_web_incremental_manifest` manifest table. The package identifies **enabled** models tagged with `snowplow_web_incremental` within your project and selects these models from the manifest in order to calculate the state of the web model as described above.
- Not simply exclude the retired models from your Snowplow web job in production. Currently the package is unable to identify which models are due to be executed in a given run. As a result, if you exclude a model the package will get stuck in State 3 as outlined in the identification of events to process section and continue to attempt to sync your excluded with the remaining models.

### Backfilling

We have created a macro `snowplow_utils.is_run_with_new_events(package_name)`, which will evaluate whether the particular model i.e. {{ this }} has already processed the events in the given run of the model. This is returned as a boolean and effectively blocks the upsert to incremental models if the run only contains old data. This protects against your derived incremental tables being temporarily updated with incomplete data during batched back-fills of other models. We recommend including this in the where clause of any incremental models you create within custom modules as follows:

```sql
select
  ...
from 
where {{ snowplow_utisl.is_run_with_new_events("snowplow_web") }}
```

A full example of this can be seen in the `custom_example` directory.

### Tips for developing custom modules

While developing custom modules you may benefit from the following:

- Minimising the amount of data being processed to reduce cost & run time.
- Use recent events from your events table to ensure you have all the latest contexts and event types available.

#### Reducing Costs

By setting `snowplow__backfill_limit_days` to 1 in your `dbt_project.yml` file you will only process a days worth of data per run.

We have provided the `get_value_by_target` macro to dynamically switch the backfill limit depending on your environment i.e. dev vs. prod, with your environment determined by your target name:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__backfill_limit_days: "{{ snowplow_utils.get_value_by_target(dev_value=1, default_value=30, dev_target_name='dev') }}"
```

#### Using Recent Data

This can be achieved by setting `snowplow__start_date` to a recent date. To dynamically change the start date depending on your environment, you can use the following:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__start_date: "{{ snowplow_utils.get_value_by_target(
                                      dev_value=snowplow_utils.n_timedeltas_ago(1, 'weeks'),
                                      default_value='2020-01-01', 
                                      dev_target_name='dev') }}"
```

Please refer to the [snowplow-utils][snowplow-utils] docs for the full documentation on these macros.

## Incremental Materialization

This package makes use of the `snowplow_incremental` materialization from the `snowplow_utils` package. This builds upon the out-of-the-box incremental materialization provided by dbt. As is the case with the native incremental materialization, the strategy varies between adapters.

Please refer to the [snowplow-utils][snowplow-utils] docs for the full documentation on `snowplow_incremental` materialization.

### Redshift and Postgres

Like the native materialization, the `snowplow_incremental` materialization strategy is delete and insert, however a limit has been imposed on how far to scan the destination table in order to improve performance:

```sql
with vars as (
  select 
    dateadd('day', -{{ snowplow__upsert_lookback_days }}, min({{ upsert_date_key }})) as lower_limit,
    max(upsert_date_key) as upper_limit
  from {{ tmp_relation }}
)

delete
from {{ destination_table }}
where {{ unique_key }} in (select {{ unique_key }} from {{ tmp_relation }})
and {{ upsert_date_key }} between (select lower_limit from vars) and (select upper_limit from vars);

insert into {{ destination_table }}
(select * from {{ tmp_relation }});
```

This materialization can be implemented in your own custom modules as follows:

```sql
{{ 
  config(
    materialized='snowplow_incremental',
    unique_key='page_veiw_id', # Required: the primary key of your model
    upsert_date_key='start_tstamp' # Required: The date key to be used to calculate the limits
  ) 
}}
```

You can equally use the default incremental dbt materialization if you would rather.

### BigQuery

Like the native materialization, the `snowplow_incremental` materialization strategy is [merge][dbt-bq-merge-strategy], however limits are calculated to allow for partition pruning on the destination table saving cost:

```sql
/*
  Create a temporary table from the model SQL
*/
create temporary table {{ model_name }}__dbt_tmp as (
  {{ model_sql }}
);

/*
  Find merge limits
*/

declare dbt_partition_lower_limit, dbt_partition_upper_limit date;
set (dbt_partition_lower_limit, dbt_partition_upper_limit) = (
    select as struct
         dateadd('day', -{{ snowplow__upsert_lookback_days }}, min({{ partition_by_key }})) as dbt_partition_lower_limit,
         max({{ partition_by_key }}) as dbt_partition_upper_limit
    from {{ model_name }}__dbt_tmp
);

/*
  Update or insert into destination table. Limit the table scan on the destination table.
*/
merge into {{ destination_table }} DEST
using {{ model_name }}__dbt_tmp SRC
on SRC.{{ unique_key }} = DEST.{{ unique_key }}
and DEST.{{ partition_by_key }} between dbt_partition_lower_limit and dbt_partition_upper_limit -- Prune partitions on DEST

when matched then update ...

when not matched then insert ...
```

This materialization can be implemented in your own custom modules as follows:

```sql
{{ 
  config(
    materialized='snowplow_incremental',
    unique_key='page_view_id', # Required: the primary key of your model
    partition_by = {
      "field": "start_tstamp",
      "data_type": "timestamp",
      "granularity": "day" # Only introduced in dbt v0.19.0+. Defaults to 'day' for dbt v0.18 or earlier
    } # Adds partitions to destination table. This field is also used to determine the upsert limits dbt_partition_lower_limit, dbt_partition_upper_limit
  ) 
}}
```

*Note you must provide the `partition_by` clause to use this materialization. All `data_types` are supported except `int64`.*

### Snowflake

Like the native materialization, the `snowplow_incremental` materialization's default strategy is [merge][dbt-snowflake-merge-strategy], however limits are calculated to allow for partition pruning on the destination table saving cost. This is performed in a similar fashion to BigQuery as outlined above.

This materialization can be implemented in your own custom modules as follows:

```sql
{{ 
  config(
    materialized='snowplow_incremental',
    unique_key='page_view_id', # Required: the primary key of your model
    upsert_date_key='start_tstamp', # Required: The date key to be used to calculate the limits
    cluster_by='to_date(start_tstamp)' # Optional.
  ) 
}}
```

If you have duplicates in your source table, you may find that the merge command returns an error. This is because duplicates produce non-deterministic results as outlined in [Snowflake's docs][snowflake-merge-duplicates]. If you experience this we suggest using the `delete+insert` strategy provided in the `snowplow_incremental` materialization.

To update all incremental models in the package to use this strategy please add the following to your `dbt_project.yml` file:

```yml
models:
  snowplow_web:
    +incremental_strategy: "delete+insert"
```

*Note: During testing we found that providing the `upsert_date_key` as a cluster key results in more effective partition pruning. This does add overhead however as the dataset needs to be sorted before being upserted. In our testing this was a worthwhile trade off, reducing overall costs. Your mileage may vary, as variables like row count can effect this.*

### Notes

- If using this the `snowplow_incremental` materialization, the native dbt `is_incremental()` macro will not recognise the model as incremental. Please use the `snowplow_utils.snowplow_is_incremental()` macro instead, which operates in the same way.
- `snowplow__upsert_lookback_days` defaults to 30 days. If you set `snowplow__upsert_lookback_days` to too short a period, duplicates can occur in your incremental table.
- If you want to remove the lookback altogether, add `disable_upsert_lookback=true,` to your config block. This effectively sets `snowplow__upsert_lookback_days` to zero for that given model.
- If you would rather use an alternative incremental materialization for all incremental models within the package, set the variable `snowplow__incremental_materialization` to your preferred materialization. See the 'Configuration' section for more details.

## Advanced Usage

### Asynchronous Runs

You may wish to run the modules asynchronously, for instance run the page views module hourly but the sessions and users modules daily. You would assume this could be achieved using:

```bash
dbt run --models +snowplow_web.page_views
```

Currently however it is not possible during a dbt jobs start phase to deduce exactly what models are due to be executed from such a command. This means the package is unable to select the subset of models from the manifest. Instead all models from the standard and custom modules are selected from the manifest and the package will attempt to synchronise all models. This makes the above command unsuitable for asynchronous runs. We can leverage dbt's `ls` in conjunction with shell substitution however to explicitly state what models to run, allowing a subset of models to be selected from the manifest and thus run the page views models independently. To run just the page views module asynchronously:

```bash
dbt run --model +snowplow_web.page_views --vars "{'models_to_run': '$(dbt ls --m  +snowplow_web.page_views --output name)'}"
```

### Cluster Keys

All the incremental models in the Snowplow web package have recommended cluster keys applied to them. Depending on your specific use case, you may want to change or disable these all together. This can be achieved by overriding the following macros with your own version within your project:

- `cluster_by_fields_sessions_lifecycle()`
- `cluster_by_fields_page_views()`
- `cluster_by_fields_sessions()`
- `cluster_by_fields_users()`

## Duplicates
This package performs de-duplication on both `event_id`'s and `page_view_id`'s, in the base and page views modules respectively. The de-duplication method for Redshift and Postgres is different to BigQuery & Snowflake due to their federated table design. The key difference between the two methodologies is that for Redshift and Postgres an `event_id` may be removed entirely during de-duplication, where as for BigQuery & Snowflake we keep all `event_id`'s. See below for a detailed explanation.

### Redshift and Postgres
Using `event_id` de-duplication as an example, for duplicates we:

- Keep the first row per `event_id` ordered by `collector_tstamp` i.e. the earliest occurring row.
- If there are multiple rows with the same `collector_tstamp`, *we discard the event all together*. This is done to avoid 1:many joins when joining on context tables such as the page view context.

The same methodology is applied to `page_view_id`s, however we order by `derived_tstamp`.

### BigQuery & Snowflake

Using `event_id` de-duplication as an example, for duplicates we:

- Keep the first row per `event_id` ordered by `collector_tstamp` i.e. the earliest occurring row.

The same methodology is applied to `page_view_id`s, however we order by `derived_tstamp`.

# Join the Snowplow community

We welcome all ideas, questions and contributions!

For support requests, please use our community support [Discourse][discourse] forum.

If you find a bug, please report an issue on GitHub.

# Copyright and license

The snowplow-web package is Copyright 2021 Snowplow Analytics Ltd.

Licensed under the [Apache License, Version 2.0][license] (the "License");
you may not use this software except in compliance with the License.

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

[license]: http://www.apache.org/licenses/LICENSE-2.0
[license-image]: http://img.shields.io/badge/license-Apache--2-blue.svg?style=flat
[tracker-classificiation]: https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/tracker-maintenance-classification/
[early-release]: https://img.shields.io/static/v1?style=flat&label=Snowplow&message=Early%20Release&color=014477&labelColor=9ba0aa&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAAAeFBMVEVMaXGXANeYANeXANZbAJmXANeUANSQAM+XANeMAMpaAJhZAJeZANiXANaXANaOAM2WANVnAKWXANZ9ALtmAKVaAJmXANZaAJlXAJZdAJxaAJlZAJdbAJlbAJmQAM+UANKZANhhAJ+EAL+BAL9oAKZnAKVjAKF1ALNBd8J1AAAAKHRSTlMAa1hWXyteBTQJIEwRgUh2JjJon21wcBgNfmc+JlOBQjwezWF2l5dXzkW3/wAAAHpJREFUeNokhQOCA1EAxTL85hi7dXv/E5YPCYBq5DeN4pcqV1XbtW/xTVMIMAZE0cBHEaZhBmIQwCFofeprPUHqjmD/+7peztd62dWQRkvrQayXkn01f/gWp2CrxfjY7rcZ5V7DEMDQgmEozFpZqLUYDsNwOqbnMLwPAJEwCopZxKttAAAAAElFTkSuQmCC

[tracker-docs]: https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/
[docs-what-is-dm]: https://docs.snowplowanalytics.com/docs/modeling-your-data/what-is-data-modeling/
[docs-data-models]: https://docs.snowplowanalytics.com/docs/modeling-your-data/
[dbt-disable-model]: https://docs.getdbt.com/reference/resource-configs/enabled#disable-a-model-in-a-package-in-order-to-use-your-own-version-of-the-model
[dbt-package-docs]: https://docs.getdbt.com/docs/building-a-dbt-project/package-management
[discourse]: http://discourse.snowplowanalytics.com/
[dbt-selectors]: https://docs.getdbt.com/reference/node-selection/yaml-selectors
[selectors-yml-file]: https://github.com/snowplow/dbt-snowplow-web/blob/main/selectors.yml
[dbt-bq-merge-strategy]: https://docs.getdbt.com/reference/resource-configs/bigquery-configs#the-merge-strategy
[dbt-snowflake-merge-strategy]: https://docs.getdbt.com/reference/resource-configs/snowflake-configs#merge-behavior-incremental-models
[snowflake-merge-duplicates]: https://docs.snowflake.com/en/sql-reference/sql/merge.html#duplicate-join-behavior
[snowplow-utils]: https://github.com/snowplow/dbt-snowplow-utils

{% endraw %}
{% enddocs %}
